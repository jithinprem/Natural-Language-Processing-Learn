{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "74cbaff1",
   "metadata": {},
   "source": [
    "# Topic Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c43f2032",
   "metadata": {},
   "source": [
    "- understand topic modelling\n",
    "- learn latent dirichlet allocation\n",
    "- implement LDA\n",
    "- Understand Non-negative Matrix factorization\n",
    "- implement NMF\n",
    "- apply LDA and NMF with a project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4597a8ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "682664cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dd2a253",
   "metadata": {},
   "source": [
    "- effeciently analyze large volumes of text by clustring document into topics\n",
    "- unlabelled data, we won't be able to apply our previous supervised learning\n",
    "- we have unlabelled data, then we can attempt to 'discover' labels\n",
    "- this means to discover clusters of documents, grouped together by topic\n",
    "- its difficult to evavulate\n",
    "- LDT allocation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82ae9ca9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "16739af3",
   "metadata": {},
   "source": [
    "LDA is based off a prob. distribution\n",
    "- assumptions\n",
    " - doucments with similar topic use similar group of words\n",
    " - latent topics can be found by searching for groups of words that freq. occour together in docs\n",
    " \n",
    " \n",
    "- assumptions of LDA for topic modelling\n",
    " - documents are probablity distributions over latent topics\n",
    " - topics themselves are probablity distribution over words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "2cabe329",
   "metadata": {},
   "outputs": [],
   "source": [
    "# what happens here is :\n",
    "## we decide how many topics we need\n",
    "## we give random topic to each words, then find out the occourances and adjust it to a common type\n",
    "## by doing so it will give a representation of the document to a topic\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42c42b93",
   "metadata": {},
   "source": [
    "- decide the number of words N the doucment will have\n",
    "- choose a topic mixture for the document (according to a Dirichlet distrib. over a fixed k topics)\n",
    "- eg : 60% business, 20% politics, 10% food\n",
    "- generate each word in the document by-\n",
    " - first picking a topic according to multinomial distribution that you sampled previously (60% business.....)\n",
    " - using the topic to generate the word itself(according to multinomial distribution)\n",
    " - eg: if we selected food, we would select the word apple with higher porb. than something like home"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62f6b33b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "766954f1",
   "metadata": {},
   "source": [
    "\n",
    "- imagine we have set of docs\n",
    "- we choosen some random fixed number k to discover.\n",
    "- go through documents and randomly assign each word in the document to one of the k topics\n",
    "- this random assignment already gives you both topic representations of all the documents and word distributions of all the topics(note, these initial random topics won't make sense)\n",
    "- we are going to iterate over everyword in every document to improve these topics\n",
    "- for every word in every document and for each topic t we calculate:\n",
    " - p(topic t | document d) = the proportion of words in document d that are currently assigned to topic t\n",
    " - p(word w | topic t) = proportion of assignments to topic t over all doc that come from this word w\n",
    "- reassign w a new topic , where we choose topic t with probablity p(topic t | document d) * p(word w | topic t)\n",
    "- this is essentially the probablity that topic t generated word w\n",
    "\n",
    "- we end up in an output such as:\n",
    " - document assigned to topic #4\n",
    " - most common words(highest probablity) for topic #4\n",
    "  - : [cat , vet, birds, dog, ..] \n",
    " - it is up to the user to interpret these to topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b486e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "257dd808",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ba409da7",
   "metadata": {},
   "source": [
    "# Latent Dirichlet alloc with python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8be1de59",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "8422a379",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "aa5b7516",
   "metadata": {},
   "outputs": [],
   "source": [
    "npr = pd.read_csv('resources/npr.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "a913b20c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Article</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>In the Washington of 2016, even when the polic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Donald Trump has used Twitter  —   his prefe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Donald Trump is unabashedly praising Russian...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Updated at 2:50 p. m. ET, Russian President Vl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>From photography, illustration and video, to d...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Article\n",
       "0  In the Washington of 2016, even when the polic...\n",
       "1    Donald Trump has used Twitter  —   his prefe...\n",
       "2    Donald Trump is unabashedly praising Russian...\n",
       "3  Updated at 2:50 p. m. ET, Russian President Vl...\n",
       "4  From photography, illustration and video, to d..."
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "npr.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "506df45d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11992"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(npr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "601867be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "e1e15744",
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_df removes the words that are in 90 (0.9 mentioned below) percent of the docs\n",
    "# if integer then its like atleast these many documents it must show up\n",
    "cv = CountVectorizer(max_df = 0.9, min_df = 2, stop_words='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "e0032bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "dtm = cv.fit_transform(npr['Article'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "6efe7b66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<11992x54777 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 3033388 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "f281b1c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "49466a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "LDA = LatentDirichletAllocation(n_components =7, random_state= 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "caba97f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LatentDirichletAllocation(n_components=7, random_state=42)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LDA.fit(dtm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "2bea4f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# grab the vocab of words\n",
    "# grab the topics\n",
    "# grab the highest probablity words per topic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf5ae09c",
   "metadata": {},
   "source": [
    "## grab the vocabulary of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18074344",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "f16e1c22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'africa'"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv.get_feature_names()[2000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "b90456e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(cv.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "2fb7a1fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "54777"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cv.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b900f7c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "acbcd81f",
   "metadata": {},
   "source": [
    "### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "911b592b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'whisked'"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random \n",
    "random_word_id = random.randint(0, 54000)\n",
    "cv.get_feature_names()[random_word_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8558c3f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "d33de8f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(LDA.components_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "bff5a62e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(LDA.components_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "5ccf21c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7, 54777)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LDA.components_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "d0f9355a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[8.64332806e+00, 2.38014333e+03, 1.42900522e-01, ...,\n",
       "        1.43006821e-01, 1.42902042e-01, 1.42861626e-01],\n",
       "       [2.76191749e+01, 5.36394437e+02, 1.42857148e-01, ...,\n",
       "        1.42861973e-01, 1.42857147e-01, 1.42906875e-01],\n",
       "       [7.22783888e+00, 8.24033986e+02, 1.42857148e-01, ...,\n",
       "        6.14236247e+00, 2.14061364e+00, 1.42923753e-01],\n",
       "       ...,\n",
       "       [3.11488651e+00, 3.50409655e+02, 1.42857147e-01, ...,\n",
       "        1.42859912e-01, 1.42857146e-01, 1.42866614e-01],\n",
       "       [4.61486388e+01, 5.14408600e+01, 3.14281373e+00, ...,\n",
       "        1.43107628e-01, 1.43902481e-01, 2.14271779e+00],\n",
       "       [4.93991422e-01, 4.18841042e+02, 1.42857151e-01, ...,\n",
       "        1.42857146e-01, 1.43760101e-01, 1.42866201e-01]])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LDA.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "600751ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "single_topic = LDA.components_[0]\n",
    "second_topic = LDA.components_[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "58e7f2a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "54777"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(single_topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "59cfd87e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2475, 18302, 35285, ..., 22673, 42561, 42993], dtype=int64)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "single_topic.argsort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "aff4af2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([33390, 36310, 21228, 10425, 31464,  8149, 36283, 22673, 42561,\n",
       "       42993], dtype=int64)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ARGSORT ---> INDEX POSITION SORTED FROM LEAST ---> GREATEST\n",
    "# TOP 10 VALUES (10 GREATEST VALUES)\n",
    "# LAST 10 VALUES OF ARGSORT()\n",
    "\n",
    "single_topic.argsort()[-10:] # grab the last 10 values of argsort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "0fb8c11b",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_ten_words = single_topic.argsort()[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "1357651d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new\n",
      "percent\n",
      "government\n",
      "company\n",
      "million\n",
      "care\n",
      "people\n",
      "health\n",
      "said\n",
      "says\n"
     ]
    }
   ],
   "source": [
    "for index in top_ten_words:\n",
    "    print(cv.get_feature_names()[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb90da5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "a18aef18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRAB THE HIGHEST PROBABLITY WORDS PER TOPIC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "92c3f06e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "THE TOP 15 WORDS FOR TOPIC #0\n",
      "['companies', 'money', 'year', 'federal', '000', 'new', 'percent', 'government', 'company', 'million', 'care', 'people', 'health', 'said', 'says']\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "THE TOP 15 WORDS FOR TOPIC #1\n",
      "['military', 'house', 'security', 'russia', 'government', 'npr', 'reports', 'says', 'news', 'people', 'told', 'police', 'president', 'trump', 'said']\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "THE TOP 15 WORDS FOR TOPIC #2\n",
      "['way', 'world', 'family', 'home', 'day', 'time', 'water', 'city', 'new', 'years', 'food', 'just', 'people', 'like', 'says']\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "THE TOP 15 WORDS FOR TOPIC #3\n",
      "['time', 'new', 'don', 'years', 'medical', 'disease', 'patients', 'just', 'children', 'study', 'like', 'women', 'health', 'people', 'says']\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "THE TOP 15 WORDS FOR TOPIC #4\n",
      "['voters', 'vote', 'election', 'party', 'new', 'obama', 'court', 'republican', 'campaign', 'people', 'state', 'president', 'clinton', 'said', 'trump']\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "THE TOP 15 WORDS FOR TOPIC #5\n",
      "['years', 'going', 've', 'life', 'don', 'new', 'way', 'music', 'really', 'time', 'know', 'think', 'people', 'just', 'like']\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "THE TOP 15 WORDS FOR TOPIC #6\n",
      "['student', 'years', 'data', 'science', 'university', 'people', 'time', 'schools', 'just', 'education', 'new', 'like', 'students', 'school', 'says']\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, topic in enumerate(LDA.components_):\n",
    "    print(f\"THE TOP 15 WORDS FOR TOPIC #{i}\")\n",
    "    print([cv.get_feature_names()[index] for index in topic.argsort()[-15:]])\n",
    "    print('\\n')\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "021c4b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_result = LDA.transform(dtm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "1141a3de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.02, 0.68, 0.  , 0.  , 0.3 , 0.  , 0.  ])"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_result[0].round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "355fa2a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_result[0].argmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "6c1c9d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "npr['Topic'] = topic_result.argmax(axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "d125da89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Article</th>\n",
       "      <th>Topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>In the Washington of 2016, even when the polic...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Donald Trump has used Twitter  —   his prefe...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Donald Trump is unabashedly praising Russian...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Updated at 2:50 p. m. ET, Russian President Vl...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>From photography, illustration and video, to d...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11987</th>\n",
       "      <td>The number of law enforcement officers shot an...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11988</th>\n",
       "      <td>Trump is busy these days with victory tours,...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11989</th>\n",
       "      <td>It’s always interesting for the Goats and Soda...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11990</th>\n",
       "      <td>The election of Donald Trump was a surprise to...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11991</th>\n",
       "      <td>Voters in the English city of Sunderland did s...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11992 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 Article  Topic\n",
       "0      In the Washington of 2016, even when the polic...      1\n",
       "1        Donald Trump has used Twitter  —   his prefe...      1\n",
       "2        Donald Trump is unabashedly praising Russian...      1\n",
       "3      Updated at 2:50 p. m. ET, Russian President Vl...      1\n",
       "4      From photography, illustration and video, to d...      2\n",
       "...                                                  ...    ...\n",
       "11987  The number of law enforcement officers shot an...      1\n",
       "11988    Trump is busy these days with victory tours,...      4\n",
       "11989  It’s always interesting for the Goats and Soda...      3\n",
       "11990  The election of Donald Trump was a surprise to...      4\n",
       "11991  Voters in the English city of Sunderland did s...      0\n",
       "\n",
       "[11992 rows x 2 columns]"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "npr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a278d0f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bc719dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bd76f603",
   "metadata": {},
   "source": [
    "## Non negative matrix Factorization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e85ebc5",
   "metadata": {},
   "source": [
    "- non supervised\n",
    "- simentaneously performs dimensionality reduction and clustring (giving a topic i guess)\n",
    "- we can use it in conjuction with TF-IDF to model topics across documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5b28859",
   "metadata": {},
   "source": [
    "- input --> non-negative data matrix A , here TFIDF\n",
    "- number of basis vectors K (refers to how many topics we want)\n",
    "- k dimensional factorization interms of W and H\n",
    "- initial values for factors W and H (eg. random matrices)\n",
    "\n",
    "\n",
    "\n",
    "- maybe ... doubt\n",
    " - n * m ---> A (data matrix --> rows=features, cols=Objects)\n",
    " - n * k ---> W (Basis Vectors --> Rows = features)\n",
    " - k * m ---> H (Coeffecient Matrix --> Cols=Objects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0badb2a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31b58857",
   "metadata": {},
   "source": [
    "- get some measure of error btw A and approximation WH\n",
    " - 1/2 * (A - WH)^2\n",
    "- optimize, expectation maximation\n",
    "- updating H, updating W until converge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "582af401",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e78fe9fe",
   "metadata": {},
   "source": [
    "### How its gonna work for US"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58ee0c70",
   "metadata": {},
   "source": [
    "- construct vector space model for document(after stopword filtering)\n",
    "- result is a term documnent matrix A\n",
    "- apply TFIDF term weight normalization to A\n",
    "- Normalize TF-IDF vectors to unit length\n",
    "- initialise factors using NNDSVD on A\n",
    "- Apply projected Gradient NMF to A\n",
    "- basis vectors: the topics(clusters) in the data\n",
    "- Coefficient matrix: the membership weights for documents relative to each topic(cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edf63730",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a30a3aa8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "091e7602",
   "metadata": {},
   "source": [
    "## Non neg- matrix factorization 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "6f2a0914",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "5af6b1a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "npr = pd.read_csv('resources/npr.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "872a990e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing feature extraction changes - \n",
    "## we could only use countvector for lda, but here we could preprocess with vector factorization..\n",
    "## ^the above statement is because maybe in LDA we needed to deal with probablity so someting like \n",
    "## but here we are optimizing the coeffecient, the coeffecient that decides to which it belongs, so \n",
    "## it may only require some sort of weight that's associated with it\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "5ecdb2bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "1b688fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(max_df = 0.95, min_df = 2, stop_words ='english' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "9f03f657",
   "metadata": {},
   "outputs": [],
   "source": [
    "dtm = tfidf.fit_transform(npr['Article'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "952f26c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<11992x54777 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 3033388 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "a4e43281",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "3c517d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "nmf_model = NMF(n_components = 7, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "a86630e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\sklearn\\decomposition\\_nmf.py:312: FutureWarning: The 'init' value, when 'init=None' and n_components is less than n_samples and n_features, will be changed from 'nndsvd' to 'nndsvda' in 1.1 (renaming of 0.26).\n",
      "  warnings.warn((\"The 'init' value, when 'init=None' and \"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "NMF(n_components=7, random_state=42)"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nmf_model.fit(dtm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "7a1f843a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bask'"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf.get_feature_names()[5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "4434f713",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "THE TOP 15 WORDS FOR TOPIC - #0\n",
      "['new', 'research', 'like', 'patients', 'health', 'disease', 'percent', 'women', 'virus', 'study', 'water', 'food', 'people', 'zika', 'says']\n",
      "\n",
      "\n",
      "\n",
      "THE TOP 15 WORDS FOR TOPIC - #1\n",
      "['gop', 'pence', 'presidential', 'russia', 'administration', 'election', 'republican', 'obama', 'white', 'house', 'donald', 'campaign', 'said', 'president', 'trump']\n",
      "\n",
      "\n",
      "\n",
      "THE TOP 15 WORDS FOR TOPIC - #2\n",
      "['senate', 'house', 'people', 'act', 'law', 'tax', 'plan', 'republicans', 'affordable', 'obamacare', 'coverage', 'medicaid', 'insurance', 'care', 'health']\n",
      "\n",
      "\n",
      "\n",
      "THE TOP 15 WORDS FOR TOPIC - #3\n",
      "['officers', 'syria', 'security', 'department', 'law', 'isis', 'russia', 'government', 'state', 'attack', 'president', 'reports', 'court', 'said', 'police']\n",
      "\n",
      "\n",
      "\n",
      "THE TOP 15 WORDS FOR TOPIC - #4\n",
      "['primary', 'cruz', 'election', 'democrats', 'percent', 'party', 'delegates', 'vote', 'state', 'democratic', 'hillary', 'campaign', 'voters', 'sanders', 'clinton']\n",
      "\n",
      "\n",
      "\n",
      "THE TOP 15 WORDS FOR TOPIC - #5\n",
      "['love', 've', 'don', 'album', 'way', 'time', 'song', 'life', 'really', 'know', 'people', 'think', 'just', 'music', 'like']\n",
      "\n",
      "\n",
      "\n",
      "THE TOP 15 WORDS FOR TOPIC - #6\n",
      "['teacher', 'state', 'high', 'says', 'parents', 'devos', 'children', 'college', 'kids', 'teachers', 'student', 'education', 'schools', 'school', 'students']\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for index, topic in enumerate(nmf_model.components_):\n",
    "    print(f'THE TOP 15 WORDS FOR TOPIC - #{index}')\n",
    "    print([tfidf.get_feature_names()[i] for i in topic.argsort()[-15:]])\n",
    "    print('\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "17766944",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_results = nmf_model.transform(dtm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "ecfa0e91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, ..., 0, 4, 3], dtype=int64)"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_results.argmax(axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "3ca483c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "npr['Topic']= topic_results.argmax(axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "3f9da7c8",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Article</th>\n",
       "      <th>Topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>In the Washington of 2016, even when the polic...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Donald Trump has used Twitter  —   his prefe...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Donald Trump is unabashedly praising Russian...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Updated at 2:50 p. m. ET, Russian President Vl...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>From photography, illustration and video, to d...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Article  Topic\n",
       "0  In the Washington of 2016, even when the polic...      1\n",
       "1    Donald Trump has used Twitter  —   his prefe...      1\n",
       "2    Donald Trump is unabashedly praising Russian...      1\n",
       "3  Updated at 2:50 p. m. ET, Russian President Vl...      3\n",
       "4  From photography, illustration and video, to d...      6"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "npr.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "11dc0eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_dic = {6:'edu', 5:'music', 4:'election', 3:'poli', 2:'legis', 2:'ele', 1:'health'} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "054b0fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "npr['Topic label'] = npr['Topic'].map(topic_dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "8b52f153",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Article</th>\n",
       "      <th>Topic</th>\n",
       "      <th>Topic label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>In the Washington of 2016, even when the polic...</td>\n",
       "      <td>1</td>\n",
       "      <td>health</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Donald Trump has used Twitter  —   his prefe...</td>\n",
       "      <td>1</td>\n",
       "      <td>health</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Donald Trump is unabashedly praising Russian...</td>\n",
       "      <td>1</td>\n",
       "      <td>health</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Updated at 2:50 p. m. ET, Russian President Vl...</td>\n",
       "      <td>3</td>\n",
       "      <td>poli</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>From photography, illustration and video, to d...</td>\n",
       "      <td>6</td>\n",
       "      <td>edu</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Article  Topic Topic label\n",
       "0  In the Washington of 2016, even when the polic...      1      health\n",
       "1    Donald Trump has used Twitter  —   his prefe...      1      health\n",
       "2    Donald Trump is unabashedly praising Russian...      1      health\n",
       "3  Updated at 2:50 p. m. ET, Russian President Vl...      3        poli\n",
       "4  From photography, illustration and video, to d...      6         edu"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "npr.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc054d25",
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99907436",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b75cee3f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c2db3ff1",
   "metadata": {},
   "source": [
    "# Assesment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d72399b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pandas and open"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7bab0ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "379dc4a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('resources/quora_questions.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0c0a4b38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Question</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is the step by step guide to invest in sh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What is the story of Kohinoor (Koh-i-Noor) Dia...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>How can I increase the speed of my internet co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Why am I mentally very lonely? How can I solve...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Which one dissolve in water quikly sugar, salt...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Question\n",
       "0  What is the step by step guide to invest in sh...\n",
       "1  What is the story of Kohinoor (Koh-i-Noor) Dia...\n",
       "2  How can I increase the speed of my internet co...\n",
       "3  Why am I mentally very lonely? How can I solve...\n",
       "4  Which one dissolve in water quikly sugar, salt..."
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cc1b8bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing , tfidf vectorization to create a vectorized document term matrix\n",
    "# use same parameters as previous lecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0d43b991",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0135a135",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(max_df = 0.95, min_df = 2, stop_words = 'english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c55ed420",
   "metadata": {},
   "outputs": [],
   "source": [
    "dtm = tfidf.fit_transform(df['Question'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "88abf2cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<404289x38669 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 2002912 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57909fdf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a2ed2e4d",
   "metadata": {},
   "source": [
    "## Non negative matrix factorization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72ab8dd9",
   "metadata": {},
   "source": [
    "- using NMF , 20 topics, random_state = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5c65ed72",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d59c786d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NMF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bde0fcb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NMF(n_components = 20, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f2ef6eea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\sklearn\\decomposition\\_nmf.py:312: FutureWarning: The 'init' value, when 'init=None' and n_components is less than n_samples and n_features, will be changed from 'nndsvd' to 'nndsvda' in 1.1 (renaming of 0.26).\n",
      "  warnings.warn((\"The 'init' value, when 'init=None' and \"\n",
      "C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\sklearn\\decomposition\\_nmf.py:1090: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n",
      "  warnings.warn(\"Maximum number of iterations %d reached. Increase it to\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "NMF(n_components=20, random_state=42)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(dtm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2ffd8ee2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.00000000e+00, 5.63036920e-02, 5.40156715e-05, ...,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "       [1.23892290e-03, 0.00000000e+00, 3.45251649e-05, ...,\n",
       "        0.00000000e+00, 3.65013292e-03, 0.00000000e+00],\n",
       "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "       ...,\n",
       "       [4.07891142e-04, 4.92671304e-03, 0.00000000e+00, ...,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "       [7.84654148e-05, 4.54449173e-04, 6.05797981e-05, ...,\n",
       "        1.70479939e-03, 0.00000000e+00, 1.70479939e-03],\n",
       "       [3.45021413e-04, 0.00000000e+00, 4.81932600e-06, ...,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "41ef4203",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "words belong to topic 0\n",
      "['phone', 'buy', 'laptop', 'movie', 'ways', '2016', 'books', 'book', 'movies', 'best']\n",
      "\n",
      "\n",
      "\n",
      "words belong to topic 1\n",
      "['use', 'exist', 'really', 'compare', 'cost', 'long', 'feel', 'work', 'mean', 'does']\n",
      "\n",
      "\n",
      "\n",
      "words belong to topic 2\n",
      "['improvement', 'delete', 'asked', 'google', 'answers', 'answer', 'ask', 'question', 'questions', 'quora']\n",
      "\n",
      "\n",
      "\n",
      "words belong to topic 3\n",
      "['internet', 'free', 'home', 'easy', 'youtube', 'ways', 'earn', 'online', 'make', 'money']\n",
      "\n",
      "\n",
      "\n",
      "words belong to topic 4\n",
      "['live', 'want', 'change', 'moment', 'real', 'important', 'thing', 'meaning', 'purpose', 'life']\n",
      "\n",
      "\n",
      "\n",
      "words belong to topic 5\n",
      "['china', 'business', 'country', 'olympics', 'available', 'job', 'spotify', 'war', 'pakistan', 'india']\n",
      "\n",
      "\n",
      "\n",
      "words belong to topic 6\n",
      "['hacking', 'want', 'python', 'languages', 'java', 'learning', 'start', 'language', 'programming', 'learn']\n",
      "\n",
      "\n",
      "\n",
      "words belong to topic 7\n",
      "['vote', 'better', 'election', 'did', 'win', 'hillary', 'president', 'clinton', 'donald', 'trump']\n",
      "\n",
      "\n",
      "\n",
      "words belong to topic 8\n",
      "['place', 'pakistan', 'happen', 'end', 'country', 'iii', 'start', 'did', 'war', 'world']\n",
      "\n",
      "\n",
      "\n",
      "words belong to topic 9\n",
      "['culture', 'women', 'work', 'girls', 'live', 'girl', 'look', 'sex', 'feel', 'like']\n",
      "\n",
      "\n",
      "\n",
      "words belong to topic 10\n",
      "['business', 'read', 'start', 'job', 'work', 'engineering', 'ways', 'bad', 'books', 'good']\n",
      "\n",
      "\n",
      "\n",
      "words belong to topic 11\n",
      "['government', 'ban', 'banning', 'black', 'indian', 'rupee', 'rs', '1000', 'notes', '500']\n",
      "\n",
      "\n",
      "\n",
      "words belong to topic 12\n",
      "['girl', '2017', 'year', 'don', 'employees', 'going', 'day', 'things', 'new', 'know']\n",
      "\n",
      "\n",
      "\n",
      "words belong to topic 13\n",
      "['language', 'fluently', 'speak', 'communication', 'pronunciation', 'speaking', 'writing', 'skills', 'improve', 'english']\n",
      "\n",
      "\n",
      "\n",
      "words belong to topic 14\n",
      "['pounds', 'reduce', 'quickly', 'loss', 'fast', 'fat', 'ways', 'gain', 'lose', 'weight']\n",
      "\n",
      "\n",
      "\n",
      "words belong to topic 15\n",
      "['person', 'machine', 'movies', 'favorite', 'job', 'home', 'sex', 'possible', 'travel', 'time']\n",
      "\n",
      "\n",
      "\n",
      "words belong to topic 16\n",
      "['tell', 'forget', 'really', 'friend', 'true', 'know', 'person', 'girl', 'fall', 'love']\n",
      "\n",
      "\n",
      "\n",
      "words belong to topic 17\n",
      "['increase', 'painless', 'instagram', 'account', 'best', 'commit', 'fastest', 'suicide', 'easiest', 'way']\n",
      "\n",
      "\n",
      "\n",
      "words belong to topic 18\n",
      "['better', 'job', 'use', 'account', 'data', 'software', 'science', 'computer', 'engineering', 'difference']\n",
      "\n",
      "\n",
      "\n",
      "words belong to topic 19\n",
      "['mind', 'google', 'flat', 'questions', 'hate', 'believe', 'ask', 'don', 'think', 'people']\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, topic in enumerate(model.components_):\n",
    "    print(f'words belong to topic {i}')\n",
    "    print([tfidf.get_feature_names()[i] for i in topic.argsort()[-10:] ])\n",
    "    print('\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20604371",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
